{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88716432",
   "metadata": {},
   "source": [
    "# NEURAL MACHINE TRANSLATION - GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb4bb18",
   "metadata": {},
   "source": [
    "## Required Module & Config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a89bf9e4-311d-4af8-846b-ccfcb9e940b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:06:05.703509Z",
     "start_time": "2024-07-16T19:06:04.086993Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import NAdam\n",
    "\n",
    "from src.Tokenizer import Corpus, LangData, dataLoader\n",
    "from src.utils import load_config, get_device, train_model\n",
    "\n",
    "from src.Normalizer import preprocess_data\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e500dcfdf8cad39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:06:05.717503Z",
     "start_time": "2024-07-16T19:06:05.704895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Loading config file\n",
    "config = load_config()\n",
    "# Get device : GPU/MPS Back-End/CPU\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4461b",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa220702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:06:05.720175Z",
     "start_time": "2024-07-16T19:06:05.718420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for english!\n",
      "Done for afrikaans!\n",
      "Done for english!\n",
      "Done for afrikaans!\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_DATA\n",
    "preprocess_data(config.TRAIN_RAW, config.TRAIN_DATA, config.TRAIN_SOURCE, \"english\")\n",
    "preprocess_data(config.TRAIN_RAW, config.TRAIN_DATA, config.TRAIN_TARGET, \"afrikaans\")\n",
    "\n",
    "# VAL_DATA\n",
    "preprocess_data(config.VAL_RAW, config.VAL_DATA, config.VAL_SOURCE, \"english\")\n",
    "preprocess_data(config.VAL_RAW, config.VAL_DATA, config.VAL_TARGET, \"afrikaans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76325a3f",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083f4d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:06:05.946750Z",
     "start_time": "2024-07-16T19:06:05.924440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder-Source\n",
    "english_data = Corpus(f\"{config.TRAIN_DATA}/english.txt\", \"English\")\n",
    "afrikaans_data = Corpus(f\"{config.TRAIN_DATA}/afrikaans.txt\", \"Afrikaans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0142f911",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ff35e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:33.861198Z",
     "start_time": "2024-07-16T19:10:33.857667Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encoder - source\n",
    "IN_ENCODER = english_data.vocab_size\n",
    "ENCODER_EMB = 256\n",
    "\n",
    "# Decoder - target\n",
    "IN_DECODER = afrikaans_data.vocab_size\n",
    "OUT_DECODER = afrikaans_data.vocab_size\n",
    "DECODER_EMB = 256\n",
    "\n",
    "# Shared\n",
    "HIDDEN_SIZE = 1024\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "train_data = LangData(english_data, afrikaans_data)\n",
    "train_loader = dataLoader(train_data, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da759f25",
   "metadata": {},
   "source": [
    "## Set the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e061c4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:35.515761Z",
     "start_time": "2024-07-16T19:10:35.499049Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embd_size, hidden_size, num_layers, bidirectional=False) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embd_size)\n",
    "        self.gru = nn.GRU(embd_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: L x B\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded: L x B x E\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embd_size, hidden_size, num_layers, bidirectional=False) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "        d =  4 if bidirectional else 2\n",
    "        self.embedding = nn.Embedding(input_size, embd_size)\n",
    "        self.gru = nn.GRU(embd_size, hidden_size, num_layers, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size * d, input_size)  # Changed concatenation dimension\n",
    "\n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        # x: B -> 1 x B\n",
    "        embedded = self.embedding(x.unsqueeze(0))  # Embedded: 1 x B x E\n",
    "        decoded, hidden = self.gru(embedded, hidden)  # Output: 1 x B x H\n",
    "        ##############################################################################################\n",
    "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
    "        decoded = decoded.permute(1,0,2)\n",
    "        attn_scores = torch.einsum('blh,bih->bl', encoder_outputs, decoded) / np.sqrt(self.gru.hidden_size) \n",
    "        alpha = attn_scores.softmax(dim=1)  # Alpha: B x L (L - encoder output sequence length)\n",
    "        context = torch.bmm(alpha.unsqueeze(1), encoder_outputs) # Context: 1 x B x H\n",
    "        output = torch.cat((decoded.permute(1,0,2), context.permute(1,0,2)), dim=-1)  # Concatenate on hidden size dimension\n",
    "        ##############################################################################################\n",
    "        prediction = self.fc(output)  # Prediction: 1 x B x V -> B x V_out\n",
    "        return prediction.squeeze(0), hidden\n",
    "\n",
    "    \n",
    "class NeuralMachineTranslation(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_vocab_size):\n",
    "        super(NeuralMachineTranslation, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_size = target_vocab_size\n",
    "\n",
    "    def forward(self, source, target, tch_force=0.9):\n",
    "        target_len, batch_size = target.shape\n",
    "        encoder_output, hidden = self.encoder(source)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, self.target_size).to(\n",
    "            source.device\n",
    "        )\n",
    "        x = target[0]\n",
    "        hidden = torch.zeros_like(hidden)\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(x, hidden, encoder_output)\n",
    "            outputs[:, t, :] = output\n",
    "            yhat = output.softmax(1).argmax(1)\n",
    "            x = target[t] if np.random.random() < tch_force else yhat\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "def greedy_search(model, source, max_len=20):\n",
    "    end_token = 2\n",
    "    inputs = source[0]\n",
    "    sequence = [1]\n",
    "\n",
    "    encoder_out, hidden = model.encoder(source)\n",
    "    hidden = torch.zeros_like(hidden)\n",
    "    for _ in range(max_len):\n",
    "        output, hidden = model.decoder(inputs, hidden, encoder_out)\n",
    "        top1 = output.argmax(1)\n",
    "        next_token = top1.item()\n",
    "        sequence.append(next_token)\n",
    "\n",
    "        if next_token == end_token:\n",
    "            break\n",
    "\n",
    "        inputs = top1\n",
    "\n",
    "    return sequence\n",
    "\n",
    "class Translator:\n",
    "    def __init__(self, model, source_lang, target_lang, device):\n",
    "        self.model = model\n",
    "        self.source_lang = source_lang\n",
    "        self.target_lang = target_lang\n",
    "        self.device = device\n",
    "\n",
    "    def translate_sentence(self, sentence, method=\"greedy\", max_len=20):\n",
    "        text = [\n",
    "            (\n",
    "                self.source_lang.stoi[word]\n",
    "                if word in self.source_lang.stoi\n",
    "                else self.source_lang.stoi[\"<unk>\"]\n",
    "            )\n",
    "            for word in sentence.strip().split()\n",
    "        ]\n",
    "        text = torch.tensor(text, dtype=torch.long).unsqueeze(1).to(self.device)\n",
    "\n",
    "        if method == \"greedy\":\n",
    "            translated = greedy_search(self.model, text, max_len)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown method: choose between 'greedy' or 'beam'\")\n",
    "\n",
    "        return \" \".join([self.target_lang.itos[idx] for idx in translated])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04f2d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:36.821935Z",
     "start_time": "2024-07-16T19:10:36.443616Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_net = Encoder(IN_ENCODER, ENCODER_EMB, HIDDEN_SIZE, NUM_LAYERS, bidirectional=True).to(device)\n",
    "decoder_net = Decoder(IN_DECODER, DECODER_EMB, HIDDEN_SIZE, NUM_LAYERS, bidirectional=True).to(device)\n",
    "model = NeuralMachineTranslation(encoder_net, decoder_net, OUT_DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee757771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:37.188014Z",
     "start_time": "2024-07-16T19:10:37.184864Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/lucien/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Thu Jul 18 16:29:52 2024) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "train_data = LangData(english_data, afrikaans_data)\n",
    "train_loader = dataLoader(train_data, BATCH_SIZE)\n",
    "\n",
    "pad_idx = afrikaans_data.stoi['<pad>']\n",
    "criterion = CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "optimizer = NAdam(model.parameters(), LR)\n",
    "translator = Translator(model, english_data, afrikaans_data, device)\n",
    "metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcb89b48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:10:38.250678Z",
     "start_time": "2024-07-16T19:10:38.105814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <sos> \"so vind vind sukses kode gesondheid (idft) belê.\" gee gee studie studie pong pong glas afstaan omstander omstander soet soet\n",
      "Refe: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "bleu                : 0.0\n",
      "precisions          : [0.10714285714285714, 0.07407407407407407, 0.038461538461538464, 0.0]\n",
      "brevity_penalty     : 0.6751251871527363\n",
      "length_ratio        : 0.717948717948718\n",
      "translation_length  : 28\n",
      "reference_length    : 39\n"
     ]
    }
   ],
   "source": [
    "# Data used for follow-up durring training\n",
    "mytext = \"<sos> given that we represent the target output as $y\\in\\{0,1\\}$ and we have $n$ training points , we can write the negative log likelihood of the parameters as follows: <eos>\"\n",
    "ground = \"<sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\"\n",
    "predicted = translator.translate_sentence(mytext)\n",
    "bleu = metric.compute(predictions=[predicted], references=[ground])\n",
    "print(f\"Pred: {predicted}\")\n",
    "print(f\"Refe: {ground}\")\n",
    "for key, val in bleu.items():\n",
    "\tprint(f\"{key:<20}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed66bc3",
   "metadata": {},
   "source": [
    "## Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b2790009f0b8f40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:16:25.338328Z",
     "start_time": "2024-07-16T19:10:40.907324Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 20/20 [00:24<00:00,  1.23s/batch, loss=1.715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> die filter filter die kat filter die filter filter die oordragsfunksie filter die filter filter die oordragsfunksie filter die filter\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 20/20 [00:23<00:00,  1.18s/batch, loss=1.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> ons het die data , met die frekwensie frekwensie , en die frekwensie in die tyd-gebied <eos>\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 20/20 [00:23<00:00,  1.19s/batch, loss=0.941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> ons het die hele van die stelsel wat deur die volgende vergelyking beskryf word : <eos>\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 20/20 [00:23<00:00,  1.20s/batch, loss=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel voorstel en ons het ons die $2n$ log-waarskynlikheidskostefunksie voorstel voorstel en ons salaris die $2n$\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.2631388306617737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch, loss=0.424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte , dan kan ons die negatiewe voorstel as\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.6496115922927856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 20/20 [00:59<00:00,  2.95s/batch, loss=0.354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel voorstel en ons het $n$ monsters , en ons kan die negatiewe log-waarskynlikheidskostefunksie skryf as\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.3687663674354553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 20/20 [00:24<00:00,  1.20s/batch, loss=0.307]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte afrigpunte , kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.7020458579063416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 20/20 [00:24<00:00,  1.21s/batch, loss=0.280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.9091564416885376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 20/20 [00:24<00:00,  1.24s/batch, loss=0.263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.9091564416885376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 20/20 [00:23<00:00,  1.19s/batch, loss=0.246]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf\n",
      "Reference: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "BLEU Score: 0.9091564416885376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "params = {\n",
    "    \"model\": model,\n",
    "    \"train_loader\": train_loader,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"criterion\": criterion,\n",
    "    \"device\": device,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"source_test\": mytext,\n",
    "    \"target_test\": ground,\n",
    "    \"translator\": translator\n",
    "}\n",
    "\n",
    "train_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de635f30",
   "metadata": {},
   "source": [
    "## EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d82ad19d8fc56be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:07.820566Z",
     "start_time": "2024-07-16T19:16:51.693873Z"
    }
   },
   "outputs": [],
   "source": [
    "EN_STR = [[' '.join(sent)] for sent in english_data.data_str]\n",
    "AF_STR = [[' '.join(sent)] for sent in afrikaans_data.data_str]\n",
    "TRANSLATED = [translator.translate_sentence(sent[0]) for sent in EN_STR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03ba966f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <sos> \"so vind vind sukses kode gesondheid (idft) belê.\" gee gee studie studie pong pong glas afstaan omstander omstander soet soet\n",
      "Refe: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "bleu                : 0.9539226852015061\n",
      "precisions          : [0.9864416159380188, 0.9799375835934017, 0.9724237560192617, 0.9639504449485256]\n",
      "brevity_penalty     : 0.9777282411009668\n",
      "length_ratio        : 0.9779726146019375\n",
      "translation_length  : 36140\n",
      "reference_length    : 36954\n"
     ]
    }
   ],
   "source": [
    "train_metric = metric.compute(predictions=TRANSLATED, references=AF_STR)\n",
    "print(f\"Pred: {predicted}\")\n",
    "print(f\"Refe: {ground}\")\n",
    "for key, val in train_metric.items():\n",
    "\tprint(f\"{key:<20}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e88e600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:19:51.550698Z",
     "start_time": "2024-07-16T19:19:51.546555Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"{config.VAL_DATA}/english.txt\") as data:\n",
    "    english_test = data.read().strip().split(\"\\n\")\n",
    "with open(f\"{config.VAL_DATA}/afrikaans.txt\") as data:\n",
    "    afrikaans_test = data.read().strip().split(\"\\n\")\n",
    "AF_TEST = [[sent] for sent in afrikaans_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f61a7571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T19:20:03.184332Z",
     "start_time": "2024-07-16T19:19:52.172350Z"
    }
   },
   "outputs": [],
   "source": [
    "TRANSLATED_VAL = [translator.translate_sentence(sent) for sent in english_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc7979e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF_TEST_REF = [[sent] for sent in afrikaans_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3664c55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: <sos> \"so vind vind sukses kode gesondheid (idft) belê.\" gee gee studie studie pong pong glas afstaan omstander omstander soet soet\n",
      "Refe: <sos> as ons die teikenuittree voorstel as $y\\in\\{0,1\\}$ en ons $n$ afrigpunte het , dan kan ons die negatiewe log-waarskynlikheidskostefunksie skryf as: <eos>\n",
      "bleu                : 0.4044743437862439\n",
      "precisions          : [0.6454463942615601, 0.48650133956172287, 0.3625925925925926, 0.2350719279916419]\n",
      "brevity_penalty     : 1.0\n",
      "length_ratio        : 1.071948372923246\n",
      "translation_length  : 15614\n",
      "reference_length    : 14566\n"
     ]
    }
   ],
   "source": [
    "val_metric = metric.compute(predictions=TRANSLATED_VAL, references=AF_TEST_REF)\n",
    "for key, val in val_metric.items():\n",
    "\tprint(f\"{key:<20}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412c869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
